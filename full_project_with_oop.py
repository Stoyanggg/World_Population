# -*- coding: utf-8 -*-
"""Full project with OOP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/145FXuh74zs2_8QSvRzumxC-c-qY8bb_V
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from bs4 import BeautifulSoup
import requests
import re
from sqlalchemy import create_engine

class WebScrapingWebsite1 ():
  def __init__ (self, url_2010):
    self.url_2010 = url_2010

  def website_1 (self):
    response_2010 = requests.get(self.url_2010)
    soup = BeautifulSoup(response_2010.text,'html.parser')
    table_2010 = soup.find('table')
    rows = table_2010.find_all('tr')

    data_2010 = []
    for row in rows:
      cells = row.find_all('td')
      if cells:
        data_2010.append([cell.text.strip() for cell in cells])
    return data_2010

  def extract_data_website_1 (self,data_2010):

    extracted_data = []

    for row in data_2010:
      position1 = row[0]
      city_country1 = row[1]
      population1 = row[2]



      extracted_data.append([position1, city_country1, population1])

      position2 = row[3]
      city_country2 = row[4]
      population2 = row[5]


      extracted_data.append([position2, city_country2, population2])


    df_2010 = pd.DataFrame(extracted_data, columns=['Rank', 'City_country', 'Population_2010'])
    return df_2010

  def data_manipulation_site_1 (self,df_2010):
    df_2010['Population_2010'] = df_2010['Population_2010'].str.replace(',', '')
    df_2010['Rank'] = df_2010['Rank'].astype(int)
    df_2010['Population_2010'] = df_2010['Population_2010'].astype(int)
    df_2010['City_country'] = df_2010['City_country'].astype(str)
    df_2010['City'] = df_2010['City_country'].str.split(',').str.get(0)
    df_2010['Country'] = df_2010['City_country'].str.split(',').str.get(1)

    new_col = ['Rank', 'City', 'Country', 'Population_2010']
    df_2010 = df_2010[new_col]
    df_2010 = df_2010.sort_values('Rank')
    return df_2010



class WebScrapingWebsite2 ():

  def __init__ (self,url_2024):
    self.url_2024 = url_2024


  def website_2 (self):

    response_2024 = requests.get(self.url_2024)
    soup_2024 = BeautifulSoup(response_2024.text, 'html.parser')
    body_2024 = soup_2024.find('tbody')
    body_2024.find_all('td')

    table_2024 = soup_2024.find("table")

    if table_2024:
        rows = table_2024.find_all("tr")

        data_2024 = []
        for row in rows:  # Skip the header row
            cols = row.find_all("td")
            cols = [col.text.strip() for col in cols]
            data_2024.append(cols)

        columns = ['City', 'Country', 'Population_2023', 'Population_2024', 'Pct_change']

        df_2024 = pd.DataFrame(data_2024[1:], columns = columns)
        return df_2024


class WebScrapingWebsite3():
  def __init__ (self,url_2015):
    self.url_2015 = url_2015

  def website_3 (self):

    response_2015 = requests.get(self.url_2015)
    soup_2015 = BeautifulSoup(response_2015.content, 'html.parser')
    table_2015 = soup_2015.find('table')
    body_2015 = table_2015.find('tbody')
    row3 = body_2015.find_all('tr')
    data_2015 = []
    for row in row3:
      cells = row.find_all('td')

      data_2015.append([cell.text.strip() for cell in cells])


    columns_2015 = ['Rank', 'City','Population', 'Country', 'Land_area','Density']
    df_2015 = pd.DataFrame(data_2015, columns=columns_2015)
    df_2015.rename(columns= {'Population':'Population_2015'}, inplace =True)
    return df_2015

  def data_manipulation_site_3(self,df_2015):
    df_2015.City = df_2015.City.str.split(',').str.get(0)
    df_2015.City = df_2015.City.str.split('-').str.get(0)
    df_2015.Population_2015 = df_2015.Population_2015.str.replace(',','')
    df_2015.Population_2015 = df_2015.Population_2015.astype(int)
    df_2015.Rank = df_2015.Rank.astype(int)
    return df_2015


class WebScrapingWebsite4 ():
  def __init__ (self,url_git):
    self.url_git = url_git

  def web_site_4 (self):
    request_git = requests.get(self.url_git)
    soup_git = BeautifulSoup(request_git.content,'html.parser')
    table_git = soup_git.find('table')
    body_git = table_git.find('tbody')
    all_raws_git = body_git.find_all('tr')
    row_data_git = []
    for row in all_raws_git:
      all_data = row.find_all('td')
      #print(all_data)
      row_data_git.append([data.text.strip() for data in all_data])
    df_git = pd.DataFrame(row_data_git)
    df_git.drop(columns=0, inplace=True)
    # df.rename(columns=columns)
    df_git.rename(columns = {1:'Country', 2:'Alpha-2 code', 3: 'Alpha-2 code', 4: 'Numeric code', 5: 'Latitude', 6:'Longitude'}, inplace=True)
    return(df_git)



class Merge_tables():
  def table_manipulations (self,df_2015,df_2010,df_2024,df_git):
    df_full = pd.merge(df_2015,df_2010,on = 'City')
    df_full = pd.merge(df_full,df_2024, on = 'City')
    df_full.Population_2023 = df_full.Population_2023.str.replace(',','')
    df_full.Population_2024 = df_full.Population_2024.str.replace(',','')
    df_full.Population_2024 = df_full.Population_2024.astype(int)
    df_full.Population_2023 = df_full.Population_2023.astype(int)
    df_full = df_full.sort_values('Population_2024', ascending = False)
    df_full.reset_index(inplace=True)
    df_full.drop(columns = 'index', inplace = True)
    df_full = pd.merge(df_full,df_git, on = 'Country')
    df_full = df_full[['City', 'Country', 'Population_2010','Population_2015','Population_2023','Population_2024','Latitude','Longitude']]
    df_full = df_full.sort_values('Population_2024', ascending= False)
    return df_full

class SaveToDBandCSV():
    def save_to_db(self, df_full):
        table_name = 'Population'
        engine = create_engine('sqlite:///World_population.db')

        # Use the parameter df_full directly
        df_full.to_sql(name=table_name, con=engine, if_exists='replace', index=False)

        engine.dispose()
    def save_to_csv(self,df_full):
      df_full.to_csv('World_population.csv')

url_2010 = 'https://sites.ontariotechu.ca/sustainabilitytoday/urban-and-energy-systems/Worlds-largest-cities/population-projections/city-population-2010.php'
web_scraper_1 = WebScrapingWebsite1(url_2010)

data_10 = web_scraper_1.website_1()


df_2010 = web_scraper_1.extract_data_website_1(data_10)

df_2010 = web_scraper_1.data_manipulation_site_1(df_2010)
df_2010

url_2024 = 'https://worldpopulationreview.com/world-cities'
web_scraper_2  = WebScrapingWebsite2(url_2024)
df_2024 = web_scraper_2.website_2()
df_2024

url_2015 = 'https://www.worldometers.info/population/largest-cities-in-the-world/'
web_scraper_3 = WebScrapingWebsite3(url_2015)
data_2015 = web_scraper_3.website_3()
df_2015 = web_scraper_3.data_manipulation_site_3(data_2015)
df_2015

url_git = 'https://gist.github.com/tadast/8827699'
web_scraper_4 = WebScrapingWebsite4(url_git)

df_git = web_scraper_4.web_site_4()
df_git
# df_2015 = web_scraper_3.data_manipulation_site_3(data_2015)
# df_2015

full_data = Merge_tables()
df_full = full_data.table_manipulations(df_2015, df_2010, df_2024, df_git)
df_full

save_to_db = SaveToDBandCSV()

# Call the method to save the DataFrame to the database and CSV
save_to_db.save_to_db(df_full)
save_to_csv = SaveToDBandCSV()
save_to_csv.save_to_csv(df_full)
df_full

df_full

data = df_full.copy()
data

import plotly as px
px.plot(data_frame= data[:10] , x = 'Population_2024', kind = 'bar', y = 'City', title = 'Top 10 cities',
        labels={'Population_2024': 'Population in 2024', 'City': 'City'})

import plotly.express as px
px.scatter_geo(data,
                     lat='Latitude',
                     lon='Longitude',
                     hover_name='City',
                     hover_data={
                         'City':True,
                         'Country': True,
                         'Population_2024': True,
                          'Latitude': False,
                          'Longitude': False
                     },
                     size='Population_2024',
                     size_max=50,
                     title='World Population Map 2024')

px.bar(data, x='City', y='Population_2024', color='Country',
             title='Population Comparison by City in 2024',
             labels={'Population_2024': 'Population 2024', 'City': 'City'})

px.sunburst(data, path=['Country', 'City'], values='Population_2024',
                  title='Sunburst Chart of Population Distribution by Country and City (2024)',
                  labels={'Population_2024': 'Population 2024'})

df_country = data.groupby('Country', as_index=False).sum()

px.choropleth(df_country, locations='Country', locationmode='country names',
                    color='Population_2024', hover_name='Country',
                    title='Choropleth Map of Population by Country (2024)',
                    labels={'Population_2024': 'Population 2024'})

top_10_cities = data[:10]
top_10_cities.reset_index(inplace=True)
top_10_cities.drop(columns = 'index', inplace = True)
top_10_cities

from sklearn.linear_model import LinearRegression
import numpy as np
years = np.array([2010,2015,2023,2024]).reshape(-1,1)
future_years = np.array([2025,2026,2027,2028,2029]).reshape(-1,1)
model = LinearRegression()
predictions = {}
for index, row in top_10_cities.iterrows():
  city =  row['City']
  populations = row[['Population_2010', 'Population_2015', 'Population_2023', 'Population_2024']].values.reshape(-1,1)
  model.fit(years,populations)
  future_populations = model.predict(future_years)

    # Store the predictions
  predictions[city] = future_populations.flatten()

predictions_df = pd.DataFrame(predictions, index=future_years.flatten())
predictions_df.index.name = 'Year'

for city in predictions_df.columns:
  fig = px.line(predictions_df, x=predictions_df.index, y=predictions_df.columns, labels={'value': 'Forecasted Population', 'variable': 'City', 'index': 'Year'})

# Customize the layout
fig.update_layout(
    title='Forecasted Population for Cities (2025-2028)',
    xaxis_title='Year',
    yaxis_title='Forecasted Population',
    legend_title='Cities',
    xaxis=dict(
        tickmode='linear',
        tick0=2025,
        dtick=1)
)

# Show the plot
fig.show()

last_10_cities = data[-10:]
last_10_cities.reset_index(inplace=True)
last_10_cities.drop(columns = 'index', inplace = True)
last_10_cities

years = np.array([2010,2015,2023,2024]).reshape(-1,1)
future_years = np.array([2025,2026,2027,2028,2029]).reshape(-1,1)
model = LinearRegression()
predictions = {}
for index, row in last_10_cities.iterrows():
  city =  row['City']
  populations = row[['Population_2010', 'Population_2015', 'Population_2023', 'Population_2024']].values.reshape(-1,1)
  model.fit(years,populations)
  future_populations = model.predict(future_years)

    # Store the predictions
  predictions[city] = future_populations.flatten()

predictions_df = pd.DataFrame(predictions, index=future_years.flatten())
predictions_df.index.name = 'Year'

for city in predictions_df.columns:
  fig = px.line(predictions_df, x=predictions_df.index, y=predictions_df.columns, labels={'value': 'Forecasted Population', 'variable': 'City', 'index': 'Year'})

# Customize the layout
fig.update_layout(
    title='Forecasted Population for Cities (2025-2028)',
    xaxis_title='Year',
    yaxis_title='Forecasted Population',
    legend_title='Cities',
    xaxis=dict(
        tickmode='linear',
        tick0=2025,
        dtick=1)
)

# Show the plot
fig.show()



















import requests
import pandas as pd
from bs4 import BeautifulSoup
from sqlalchemy import create_engine

class WebScrapingWebsite1:
    def __init__(self, url_2010):
        self.url_2010 = url_2010

    def website_1(self):
        try:
            response_2010 = requests.get(self.url_2010)
            response_2010.raise_for_status()  # Raise an exception for HTTP errors
            soup = BeautifulSoup(response_2010.text, 'html.parser')
            table_2010 = soup.find('table')
            rows = table_2010.find_all('tr')

            data_2010 = []
            for row in rows:
                cells = row.find_all('td')
                if cells:
                    data_2010.append([cell.text.strip() for cell in cells])
            return data_2010
        except requests.RequestException as e:
            print(f"Error fetching data from {self.url_2010}: {e}")
        except Exception as e:
            print(f"An error occurred while parsing data: {e}")

    def extract_data_website_1(self, data_2010):
        try:
            extracted_data = []
            for row in data_2010:
                position1 = row[0]
                city_country1 = row[1]
                population1 = row[2]
                extracted_data.append([position1, city_country1, population1])

                position2 = row[3]
                city_country2 = row[4]
                population2 = row[5]
                extracted_data.append([position2, city_country2, population2])

            df_2010 = pd.DataFrame(extracted_data, columns=['Rank', 'City_country', 'Population_2010'])
            return df_2010
        except IndexError as e:
            print(f"Error processing data: {e}")
        except Exception as e:
            print(f"An error occurred while extracting data: {e}")

    def data_manipulation_site_1(self, df_2010):
        try:
            df_2010['Population_2010'] = df_2010['Population_2010'].str.replace(',', '')
            df_2010['Rank'] = df_2010['Rank'].astype(int)
            df_2010['Population_2010'] = df_2010['Population_2010'].astype(int)
            df_2010['City_country'] = df_2010['City_country'].astype(str)
            df_2010['City'] = df_2010['City_country'].str.split(',').str.get(0)
            df_2010['Country'] = df_2010['City_country'].str.split(',').str.get(1)

            new_col = ['Rank', 'City', 'Country', 'Population_2010']
            df_2010 = df_2010[new_col]
            df_2010 = df_2010.sort_values('Rank')
            return df_2010
        except KeyError as e:
            print(f"Error manipulating data: {e}")
        except Exception as e:
            print(f"An error occurred during data manipulation: {e}")

class WebScrapingWebsite2:
    def __init__(self, url_2024):
        self.url_2024 = url_2024

    def website_2(self):
        try:
            response_2024 = requests.get(self.url_2024)
            response_2024.raise_for_status()
            soup_2024 = BeautifulSoup(response_2024.text, 'html.parser')
            table_2024 = soup_2024.find("table")

            if table_2024:
                rows = table_2024.find_all("tr")
                data_2024 = []
                for row in rows:
                    cols = row.find_all("td")
                    cols = [col.text.strip() for col in cols]
                    data_2024.append(cols)

                columns = ['City', 'Country', 'Population_2023', 'Population_2024', 'Pct_change']
                df_2024 = pd.DataFrame(data_2024[1:], columns=columns)
                return df_2024
        except requests.RequestException as e:
            print(f"Error fetching data from {self.url_2024}: {e}")
        except Exception as e:
            print(f"An error occurred while parsing data: {e}")

class WebScrapingWebsite3:
    def __init__(self, url_2015):
        self.url_2015 = url_2015

    def website_3(self):
        try:
            response_2015 = requests.get(self.url_2015)
            response_2015.raise_for_status()
            soup_2015 = BeautifulSoup(response_2015.content, 'html.parser')
            table_2015 = soup_2015.find('table')
            body_2015 = table_2015.find('tbody')
            row3 = body_2015.find_all('tr')
            data_2015 = []
            for row in row3:
                cells = row.find_all('td')
                data_2015.append([cell.text.strip() for cell in cells])

            columns_2015 = ['Rank', 'City', 'Population', 'Country', 'Land_area', 'Density']
            df_2015 = pd.DataFrame(data_2015, columns=columns_2015)
            df_2015.rename(columns={'Population': 'Population_2015'}, inplace=True)
            return df_2015
        except requests.RequestException as e:
            print(f"Error fetching data from {self.url_2015}: {e}")
        except Exception as e:
            print(f"An error occurred while parsing data: {e}")

    def data_manipulation_site_3(self, df_2015):
        try:
            df_2015['City'] = df_2015['City'].str.split(',').str.get(0)
            df_2015['City'] = df_2015['City'].str.split('-').str.get(0)
            df_2015['Population_2015'] = df_2015['Population_2015'].str.replace(',', '')
            df_2015['Population_2015'] = df_2015['Population_2015'].astype(int)
            df_2015['Rank'] = df_2015['Rank'].astype(int)
            return df_2015
        except KeyError as e:
            print(f"Error manipulating data: {e}")
        except Exception as e:
            print(f"An error occurred during data manipulation: {e}")

class WebScrapingWebsite4:
    def __init__(self, url_git):
        self.url_git = url_git

    def web_site_4(self):
        try:
            request_git = requests.get(self.url_git)
            request_git.raise_for_status()
            soup_git = BeautifulSoup(request_git.content, 'html.parser')
            table_git = soup_git.find('table')
            body_git = table_git.find('tbody')
            all_raws_git = body_git.find_all('tr')
            row_data_git = []
            for row in all_raws_git:
                all_data = row.find_all('td')
                row_data_git.append([data.text.strip() for data in all_data])

            df_git = pd.DataFrame(row_data_git)
            df_git.drop(columns=0, inplace=True)
            df_git.rename(columns={1: 'Country', 2: 'Alpha-2 code', 3: 'Alpha-3 code', 4: 'Numeric code', 5: 'Latitude', 6: 'Longitude'}, inplace=True)
            return df_git
        except requests.RequestException as e:
            print(f"Error fetching data from {self.url_git}: {e}")
        except Exception as e:
            print(f"An error occurred while parsing data: {e}")

class MergeTables:
    def table_manipulations(self, df_2015, df_2010, df_2024, df_git):
        try:
            df_full = pd.merge(df_2015, df_2010, on='City')
            df_full = pd.merge(df_full, df_2024, on='City')
            df_full['Population_2023'] = df_full['Population_2023'].str.replace(',', '')
            df_full['Population_2024'] = df_full['Population_2024'].str.replace(',', '')
            df_full['Population_2024'] = df_full['Population_2024'].astype(int)
            df_full['Population_2023'] = df_full['Population_2023'].astype(int)
            df_full = df_full.sort_values('Population_2024', ascending=False)
            df_full = pd.merge(df_full, df_git, on='Country')
            df_full = df_full[['City', 'Country', 'Population_2010', 'Population_2015', 'Population_2023', 'Population_2024', 'Latitude', 'Longitude']]
            df_full = df_full.sort_values('Population_2024', ascending=False)
            df_full.reset_index(inplace=True)
            df_full.drop(columns='index', inplace=True)

            return df_full
        except KeyError as e:
            print(f"Error merging data: {e}")
        except Exception as e:
            print(f"An error occurred during data merging: {e}")

class SaveToDBandCSV:
    def save_to_db(self, df_full):
        try:
            table_name = 'Population'
            engine = create_engine('sqlite:///World_population.db')
            df_full.to_sql(name=table_name, con=engine, if_exists='replace', index=False)
            engine.dispose()
        except Exception as e:
            print(f"An error occurred while saving to the database: {e}")

    def save_to_csv(self, df_full):
        try:
            df_full.to_csv('World_population.csv', index=False)
        except Exception as e:
            print(f"An error occurred while saving to CSV: {e}")

# URLs for scraping
url_2010 = 'https://sites.ontariotechu.ca/sustainabilitytoday/urban-and-energy-systems/Worlds-largest-cities/population-projections/city-population-2010.php'
url_2024 = 'https://worldpopulationreview.com/world-cities'
url_2015 = 'https://www.worldometers.info/population/largest-cities-in-the-world/'
url_git = 'https://gist.github.com/tadast/8827699'

# Web scraping and data manipulation
web_scraper_1 = WebScrapingWebsite1(url_2010)
data_10 = web_scraper_1.website_1()
df_2010 = web_scraper_1.extract_data_website_1(data_10)
df_2010 = web_scraper_1.data_manipulation_site_1(df_2010)

web_scraper_2 = WebScrapingWebsite2(url_2024)
df_2024 = web_scraper_2.website_2()

web_scraper_3 = WebScrapingWebsite3(url_2015)
data_2015 = web_scraper_3.website_3()
df_2015 = web_scraper_3.data_manipulation_site_3(data_2015)

web_scraper_4 = WebScrapingWebsite4(url_git)
df_git = web_scraper_4.web_site_4()

# Merging data
full_data = MergeTables()
df_full = full_data.table_manipulations(df_2015, df_2010, df_2024, df_git)

# Saving data to DB and CSV
save_to_db = SaveToDBandCSV()
save_to_db.save_to_db(df_full)

save_to_csv = SaveToDBandCSV()
save_to_csv.save_to_csv(df_full)

df_full

data = df_full.copy()
data





def NumberSearch(input_string):
    digits_sum = 0
    letters_sum = 0

    for i in input_string:
        if i.isdigit():
            digits_sum += int(i)
        elif i.isalpha():
            letters_sum += 1

    if letters_sum == 0:
        return 0

    return digits_sum // letters_sum

if __name__ == "__main__":
    input_string = input().strip()  # Read input from the console
    result = NumberSearch(input_string)
    print(result)

result = NumberSearch('One Number*1*')
print(result)

code = input('write some')
code

def NumberSearch(input_string):
    digits_sum = sum(int(char) for char in input_string if char.isdigit())
    letters_count = sum(1 for char in input_string if char.isalpha())

    if letters_count == 0:
        return 0

    return digits_sum // letters_count

# Direct call to the function with a sample input
result = NumberSearch('One Number*1*')
print(result)

def NumberSearch():
  all_digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

# define all letters
  all_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
               'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

  total_digits = 0
  total_letters = 0
  sentence = input()
  for i in sentence:
    if i in all_digits:
      total_digits += int(i)
    elif i in all_letters :
      total_letters += 1
  print(total_digits//total_letters)

result = NumberSearch()
result

